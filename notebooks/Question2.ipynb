{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "id": "OX4rjQyfjfjS"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from keras.datasets import fashion_mnist\n",
        "def get_fa_mnist(flatten=True):\n",
        "    (x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()\n",
        "    if flatten:\n",
        "        x_train = x_train.reshape(x_train.shape[0], 28*28)\n",
        "        x_test = x_test.reshape(x_test.shape[0], 28*28)\n",
        "    x_train = x_train.astype(np.float32) / 255.0\n",
        "    x_test = x_test.astype(np.float32) / 255.0\n",
        "    y_train = np.eye(10)[y_train]\n",
        "    y_test = np.eye(10)[y_test]\n",
        "    return (x_train, y_train), (x_test, y_test)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "class Activate:\n",
        "    @staticmethod\n",
        "    def leaky_relu(z, alpha=0.01):\n",
        "        return np.where(z > 0, z, alpha*z)\n",
        "    @staticmethod\n",
        "    def leaky_relu_der(z, alpha=0.01):\n",
        "        dz = np.ones_like(z)\n",
        "        dz[z < 0] = alpha\n",
        "        return dz\n",
        "    @staticmethod\n",
        "    def soft(z):\n",
        "        exp_z = np.exp(z - np.max(z, axis=1, keepdims=True))\n",
        "        return exp_z / np.sum(exp_z, axis=1, keepdims=True)"
      ],
      "metadata": {
        "id": "Zj7zpwLmj-Cf"
      },
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FNN:\n",
        "    def __init__(self, input_size, hidden_layers, output_size):\n",
        "        np.random.seed(42)\n",
        "        self.layers = [input_size] + hidden_layers + [output_size]\n",
        "        self.weights = {}\n",
        "        self.biases = {}\n",
        "        for i in range(len(self.layers)-1):\n",
        "            limit = np.sqrt(6 / (self.layers[i] + self.layers[i+1]))\n",
        "            self.weights['W'+str(i+1)] = np.random.uniform(-limit, limit, (self.layers[i], self.layers[i+1]))\n",
        "            self.biases['b'+str(i+1)] = np.zeros((1, self.layers[i+1]))\n",
        "    def forw(self, X):\n",
        "      self.cache = {'A0': X}\n",
        "      L = len(self.layers)-1\n",
        "      for l in range(1, L):\n",
        "          Z = np.dot(self.cache['A'+str(l-1)], self.weights['W'+str(l)]) + self.biases['b'+str(l)]\n",
        "          A = Activate.leaky_relu(Z)\n",
        "          self.cache['Z'+str(l)] = Z\n",
        "          self.cache['A'+str(l)] = A\n",
        "      ZL = np.dot(self.cache['A'+str(L-1)], self.weights['W'+str(L)]) + self.biases['b'+str(L)]\n",
        "      AL = Activate.soft(ZL)\n",
        "      self.cache['Z'+str(L)] = ZL\n",
        "      self.cache['A'+str(L)] = AL\n",
        "      return AL\n",
        "    def cross_ent(self,Y_pred, Y_true):\n",
        "      m = Y_true.shape[0]\n",
        "      return np.mean(-np.sum(Y_true * np.log(Y_pred + 1e-8), axis=1))\n",
        "    def backw(self, Y_true, learning_rate=0.01):\n",
        "      grads_w = {}\n",
        "      grads_b = {}\n",
        "      L = len(self.layers)-1\n",
        "      m = Y_true.shape[0]\n",
        "      dZ = self.cache['A'+str(L)] - Y_true\n",
        "      grads_w['dW'+str(L)] = np.dot(self.cache['A'+str(L-1)].T, dZ)/m\n",
        "      grads_b['db'+str(L)] = np.sum(dZ, axis=0, keepdims=True)/m\n",
        "      dA_prev = np.dot(dZ, self.weights['W'+str(L)].T)\n",
        "      for l in reversed(range(1, L)):\n",
        "          dZ = dA_prev * Activate.leaky_relu_der(self.cache['Z'+str(l)])\n",
        "          grads_w['dW'+str(l)] = np.dot(self.cache['A'+str(l-1)].T, dZ)/m\n",
        "          grads_b['db'+str(l)] = np.sum(dZ, axis=0, keepdims=True)/m\n",
        "          if l > 1:\n",
        "              dA_prev = np.dot(dZ, self.weights['W'+str(l)].T)\n",
        "      for l in range(1, L+1):\n",
        "          self.weights['W'+str(l)] -= learning_rate * grads_w['dW'+str(l)]\n",
        "          self.biases['b'+str(l)] -= learning_rate * grads_b['db'+str(l)]\n",
        "    def train(self, X_train, Y_train, X_test, Y_test, lr=0.005, epochs=15):\n",
        "      for epoch in range(epochs):\n",
        "        Y_pred = self.forw(X_train)\n",
        "        loss = self.cross_ent(Y_pred, Y_train)\n",
        "        self.backw(Y_train, learning_rate=lr)\n",
        "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {loss:.4f}\")\n",
        "      Y_pred_test = self.forw(X_test)\n",
        "      accuracy = np.mean(np.argmax(Y_pred_test, axis=1) == np.argmax(Y_test, axis=1))\n",
        "      print(f\"Test Accuracy: {accuracy*100:.2f}%\")"
      ],
      "metadata": {
        "id": "pAxhNSYjkeXh"
      },
      "execution_count": 75,
      "outputs": []
    }
  ]
}