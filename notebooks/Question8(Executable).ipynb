{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from keras.datasets import fashion_mnist\n",
        "def get_fa_mnist(flatten=True):\n",
        "    (x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()\n",
        "    if flatten:\n",
        "        x_train = x_train.reshape(x_train.shape[0], 28*28)\n",
        "        x_test = x_test.reshape(x_test.shape[0], 28*28)\n",
        "    x_train = x_train.astype(np.float32) / 255.0\n",
        "    x_test = x_test.astype(np.float32) / 255.0\n",
        "    y_train = np.eye(10)[y_train]\n",
        "    y_test = np.eye(10)[y_test]\n",
        "    return (x_train, y_train), (x_test, y_test)"
      ],
      "metadata": {
        "id": "B_vnMHkgi4L5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q6FEGRrnguAj"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "class Activate:\n",
        "    @staticmethod\n",
        "    def leaky_relu(z, alpha=0.01):\n",
        "        return np.where(z > 0, z, alpha*z)\n",
        "    @staticmethod\n",
        "    def leaky_relu_der(z, alpha=0.01):\n",
        "        dz = np.ones_like(z)\n",
        "        dz[z < 0] = alpha\n",
        "        return dz\n",
        "    @staticmethod\n",
        "    def soft(z):\n",
        "        exp_z = np.exp(z - np.max(z, axis=1, keepdims=True))\n",
        "        return exp_z / np.sum(exp_z, axis=1, keepdims=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class FNN:\n",
        "    def __init__(self, input_size, hidden_layers, output_size):\n",
        "        np.random.seed(42)\n",
        "        self.layers = [input_size] + hidden_layers + [output_size]\n",
        "        self.weights = {}\n",
        "        self.biases = {}\n",
        "        for i in range(len(self.layers)-1):\n",
        "            limit = np.sqrt(6 / (self.layers[i] + self.layers[i+1]))\n",
        "            self.weights['W'+str(i+1)] = np.random.uniform(-limit, limit, (self.layers[i], self.layers[i+1]))\n",
        "            self.biases['b'+str(i+1)] = np.zeros((1, self.layers[i+1]))\n",
        "    def forw(self, X):\n",
        "      self.cache = {'A0': X}\n",
        "      L = len(self.layers)-1\n",
        "      for l in range(1, L):\n",
        "          Z = np.dot(self.cache['A'+str(l-1)], self.weights['W'+str(l)]) + self.biases['b'+str(l)]\n",
        "          A = Activate.leaky_relu(Z)\n",
        "          self.cache['Z'+str(l)] = Z\n",
        "          self.cache['A'+str(l)] = A\n",
        "      ZL = np.dot(self.cache['A'+str(L-1)], self.weights['W'+str(L)]) + self.biases['b'+str(L)]\n",
        "      AL = Activate.soft(ZL)\n",
        "      self.cache['Z'+str(L)] = ZL\n",
        "      self.cache['A'+str(L)] = AL\n",
        "      return AL\n",
        "    def cross_ent(self,Y_pred, Y_true):\n",
        "      m = Y_true.shape[0]\n",
        "      return np.mean(-np.sum(Y_true * np.log(Y_pred + 1e-8), axis=1))\n",
        "    def backw(self, Y_true, learning_rate=0.01):\n",
        "      grads_w = {}\n",
        "      grads_b = {}\n",
        "      L = len(self.layers)-1\n",
        "      m = Y_true.shape[0]\n",
        "      dZ = self.cache['A'+str(L)] - Y_true\n",
        "      grads_w['dW'+str(L)] = np.dot(self.cache['A'+str(L-1)].T, dZ)/m\n",
        "      grads_b['db'+str(L)] = np.sum(dZ, axis=0, keepdims=True)/m\n",
        "      dA_prev = np.dot(dZ, self.weights['W'+str(L)].T)\n",
        "      for l in reversed(range(1, L)):\n",
        "          dZ = dA_prev * Activate.leaky_relu_der(self.cache['Z'+str(l)])\n",
        "          grads_w['dW'+str(l)] = np.dot(self.cache['A'+str(l-1)].T, dZ)/m\n",
        "          grads_b['db'+str(l)] = np.sum(dZ, axis=0, keepdims=True)/m\n",
        "          if l > 1:\n",
        "              dA_prev = np.dot(dZ, self.weights['W'+str(l)].T)\n",
        "      for l in range(1, L+1):\n",
        "          self.weights['W'+str(l)] -= learning_rate * grads_w['dW'+str(l)]\n",
        "          self.biases['b'+str(l)] -= learning_rate * grads_b['db'+str(l)]\n",
        "    def train(self, X_train, Y_train, X_test, Y_test, lr=0.005, epochs=15):\n",
        "      for epoch in range(epochs):\n",
        "        Y_pred = self.forw(X_train)\n",
        "        loss = self.cross_ent(Y_pred, Y_train)\n",
        "        self.backw(Y_train, learning_rate=lr)\n",
        "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {loss:.4f}\")\n",
        "      Y_pred_test = self.forw(X_test)\n",
        "      accuracy = np.mean(np.argmax(Y_pred_test, axis=1) == np.argmax(Y_test, axis=1))\n",
        "      print(f\"Test Accuracy: {accuracy*100:.2f}%\")"
      ],
      "metadata": {
        "id": "jWNHyvbXgu80"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import wandb\n",
        "wandb.init(\n",
        "    project=\"fashion-mnist-ques8\",\n",
        "    name=\"CE_vs_SE\"\n",
        ")\n",
        "class Activations:\n",
        "    @staticmethod\n",
        "    def relu(Z):\n",
        "        return np.maximum(0, Z)\n",
        "\n",
        "    @staticmethod\n",
        "    def relu_der(Z):\n",
        "        return (Z > 0).astype(float)\n",
        "\n",
        "    @staticmethod\n",
        "    def softmax(Z):\n",
        "        exps = np.exp(Z - np.max(Z, axis=1, keepdims=True))\n",
        "        return exps / np.sum(exps, axis=1, keepdims=True)\n",
        "def categorical_cross_entropy(pred, true):\n",
        "    m = true.shape[0]\n",
        "    return -np.sum(true * np.log(pred + 1e-8)) / m\n",
        "def squared_error_loss(pred, true):\n",
        "    return np.mean(np.sum((pred - true) ** 2, axis=1))\n",
        "class NeuralNet:\n",
        "    def __init__(self, input_dim, hidden_layers, output_dim, optimizer='sgd', lr=0.01, beta1=0.9, beta2=0.999, eps=1e-8):\n",
        "        np.random.seed(99)\n",
        "        self.layers = [input_dim] + hidden_layers + [output_dim]\n",
        "        self.weights = {}\n",
        "        self.biases = {}\n",
        "        self.opt = optimizer.lower()\n",
        "        self.lr = lr\n",
        "        self.beta1 = beta1\n",
        "        self.beta2 = beta2\n",
        "        self.eps = eps\n",
        "        self.time_step = 0\n",
        "        for i in range(len(self.layers)-1):\n",
        "            limit = np.sqrt(6 / (self.layers[i] + self.layers[i+1]))\n",
        "            self.weights[f\"W{i+1}\"] = np.random.uniform(-limit, limit, (self.layers[i], self.layers[i+1]))\n",
        "            self.biases[f\"b{i+1}\"] = np.zeros((1, self.layers[i+1]))\n",
        "        self.velocity_w = {k: np.zeros_like(v) for k,v in self.weights.items()}\n",
        "        self.velocity_b = {k: np.zeros_like(v) for k,v in self.biases.items()}\n",
        "        self.sqr_w = {k: np.zeros_like(v) for k,v in self.weights.items()}\n",
        "        self.sqr_b = {k: np.zeros_like(v) for k,v in self.biases.items()}\n",
        "    def forward(self, X):\n",
        "        store = {'A0': X}\n",
        "        L = len(self.layers) - 1\n",
        "        for l in range(1, L):\n",
        "            Z = np.dot(store[f\"A{l-1}\"], self.weights[f\"W{l}\"]) + self.biases[f\"b{l}\"]\n",
        "            A = Activations.relu(Z)\n",
        "            store[f\"Z{l}\"], store[f\"A{l}\"] = Z, A\n",
        "\n",
        "        ZL = np.dot(store[f\"A{L-1}\"], self.weights[f\"W{L}\"]) + self.biases[f\"b{L}\"]\n",
        "        AL = Activations.softmax(ZL)\n",
        "        store[f\"Z{L}\"], store[f\"A{L}\"] = ZL, AL\n",
        "        return AL, store\n",
        "    def backward(self, Y_pred, Y_true, store, loss_type=\"cross_entropy\"):\n",
        "        grads = {}\n",
        "        L = len(self.layers)-1\n",
        "        m = Y_true.shape[0]\n",
        "        if loss_type == \"cross_entropy\":\n",
        "          dZ = Y_pred - Y_true\n",
        "        elif loss_type == \"squared_error\":\n",
        "          dZ = (Y_pred - Y_true) * Y_pred * (1 - Y_pred)\n",
        "        grads[f\"dW{L}\"] = np.dot(store[f\"A{L-1}\"].T, dZ)/m\n",
        "        grads[f\"db{L}\"] = np.sum(dZ, axis=0, keepdims=True)/m\n",
        "        dA_prev = np.dot(dZ, self.weights[f\"W{L}\"].T)\n",
        "        for l in reversed(range(1, L)):\n",
        "            dZ = dA_prev * Activations.relu_der(store[f\"Z{l}\"])\n",
        "            grads[f\"dW{l}\"] = np.dot(store[f\"A{l-1}\"].T, dZ)/m\n",
        "            grads[f\"db{l}\"] = np.sum(dZ, axis=0, keepdims=True)/m\n",
        "            if l > 1:\n",
        "                dA_prev = np.dot(dZ, self.weights[f\"W{l}\"].T)\n",
        "        return grads\n",
        "    def update_params(self, grads):\n",
        "        self.time_step += 1\n",
        "        for l in range(1, len(self.layers)):\n",
        "            Wk, bk = f\"W{l}\", f\"b{l}\"\n",
        "            dW, db = grads[f\"dW{l}\"], grads[f\"db{l}\"]\n",
        "\n",
        "            if self.opt == 'sgd':\n",
        "                self.weights[Wk] -= self.lr * dW\n",
        "                self.biases[bk] -= self.lr * db\n",
        "            elif self.opt == 'momentum':\n",
        "                self.velocity_w[Wk] = self.beta1*self.velocity_w[Wk] + (1-self.beta1)*dW\n",
        "                self.velocity_b[bk] = self.beta1*self.velocity_b[bk] + (1-self.beta1)*db\n",
        "                self.weights[Wk] -= self.lr * self.velocity_w[Wk]\n",
        "                self.biases[bk] -= self.lr * self.velocity_b[bk]\n",
        "            elif self.opt == 'nesterov':\n",
        "              vW_prev = self.velocity_w[Wk]\n",
        "              vb_prev = self.velocity_b[bk]\n",
        "              self.velocity_w[Wk] = self.beta1 * self.velocity_w[Wk] - self.lr * dW\n",
        "              self.velocity_b[bk] = self.beta1 * self.velocity_b[bk] - self.lr * db\n",
        "              self.weights[Wk] += -self.beta1 * vW_prev + (1 + self.beta1) * self.velocity_w[Wk]\n",
        "              self.biases[bk]  += -self.beta1 * vb_prev + (1 + self.beta1) * self.velocity_b[bk]\n",
        "            elif self.opt == 'rmsprop':\n",
        "                self.sqr_w[Wk] = self.beta2*self.sqr_w[Wk] + (1-self.beta2)*(dW**2)\n",
        "                self.sqr_b[bk] = self.beta2*self.sqr_b[bk] + (1-self.beta2)*(db**2)\n",
        "                self.weights[Wk] -= self.lr * dW / (np.sqrt(self.sqr_w[Wk])+self.eps)\n",
        "                self.biases[bk] -= self.lr * db / (np.sqrt(self.sqr_b[bk])+self.eps)\n",
        "            elif self.opt == 'adam':\n",
        "                self.velocity_w[Wk] = self.beta1*self.velocity_w[Wk] + (1-self.beta1)*dW\n",
        "                self.velocity_b[bk] = self.beta1*self.velocity_b[bk] + (1-self.beta1)*db\n",
        "                self.sqr_w[Wk] = self.beta2*self.sqr_w[Wk] + (1-self.beta2)*(dW**2)\n",
        "                self.sqr_b[bk] = self.beta2*self.sqr_b[bk] + (1-self.beta2)*(db**2)\n",
        "                vW_corr = self.velocity_w[Wk]/(1-self.beta1**self.time_step)\n",
        "                vb_corr = self.velocity_b[bk]/(1-self.beta1**self.time_step)\n",
        "                sW_corr = self.sqr_w[Wk]/(1-self.beta2**self.time_step)\n",
        "                sb_corr = self.sqr_b[bk]/(1-self.beta2**self.time_step)\n",
        "                self.weights[Wk] -= self.lr * vW_corr / (np.sqrt(sW_corr)+self.eps)\n",
        "                self.biases[bk] -= self.lr * vb_corr / (np.sqrt(sb_corr)+self.eps)\n",
        "            elif self.opt == 'nadam':\n",
        "                 self.velocity_w[Wk] = self.beta1*self.velocity_w[Wk] + (1-self.beta1)*dW\n",
        "                 self.velocity_b[bk] = self.beta1*self.velocity_b[bk] + (1-self.beta1)*db\n",
        "                 m_hat_b = self.velocity_b[bk] / (1 - self.beta1**self.time_step)\n",
        "                 vW_hat = (self.velocity_w[Wk]*self.beta1 + (1-self.beta1)*dW) / (1-self.beta1**self.time_step)\n",
        "                 vb_hat = (self.velocity_b[bk]*self.beta1 + (1-self.beta1)*db) / (1-self.beta1**self.time_step)\n",
        "                 self.sqr_w[Wk] = self.beta2*self.sqr_w[Wk] + (1-self.beta2)*(dW**2)\n",
        "                 self.sqr_b[bk] = self.beta2*self.sqr_b[bk] + (1-self.beta2)*(db**2)\n",
        "                 sW_corr = self.sqr_w[Wk]/(1-self.beta2**self.time_step)\n",
        "                 sb_corr = self.sqr_b[bk]/(1-self.beta2**self.time_step)\n",
        "                 self.weights[Wk] -= self.lr * vW_hat / (np.sqrt(sW_corr)+self.eps)\n",
        "                 self.biases[bk] -= self.lr * vb_hat / (np.sqrt(sb_corr)+self.eps)\n",
        "    def train(self, X_train, Y_train, X_test, Y_test, epochs=20, batch_size=64):\n",
        "        m = X_train.shape[0]\n",
        "        loss_ce = []\n",
        "        loss_se = []\n",
        "        for ep in range(epochs):\n",
        "            perm = np.random.permutation(m)\n",
        "            X_shuf, Y_shuf = X_train[perm], Y_train[perm]\n",
        "            for i in range(0, m, batch_size):\n",
        "                Xb, Yb = X_shuf[i:i+batch_size], Y_shuf[i:i+batch_size]\n",
        "                Y_pred, store = self.forward(Xb)\n",
        "                grads = self.backward(Y_pred, Yb, store)\n",
        "                self.update_params(grads)\n",
        "            epoch=ep\n",
        "            preds, store = self.forward(X_train)\n",
        "            ce = categorical_cross_entropy(preds, Y_train)\n",
        "            se = squared_error_loss(preds, Y_train)\n",
        "            preds_test, _ = self.forward(X_test)\n",
        "            test_acc = np.mean(np.argmax(preds_test, axis=1) == np.argmax(Y_test, axis=1))\n",
        "            loss_ce.append(ce)\n",
        "            loss_se.append(se)\n",
        "            wandb.log({\n",
        "                \"epoch\": epoch,\n",
        "                \"cross_entropy_loss\": ce,\n",
        "                \"squared_error_loss\": se,\n",
        "                \"test_accuracy\": test_acc\n",
        "            })\n",
        "            print(\n",
        "                f\"Epoch {ep+1}/{epochs}, \"\n",
        "                f\"CE Loss: {ce:.4f}, \"\n",
        "                f\"SE Loss: {se:.4f}\"\n",
        "            )\n",
        "        wandb.log({\n",
        "            \"loss_comparison\": wandb.plot.line_series(\n",
        "            xs=list(range(len(loss_ce))),\n",
        "            ys=[loss_ce, loss_se],\n",
        "            keys=[\"Cross Entropy\", \"Squared Error\"],\n",
        "            title=\"Loss Function Comparison\",\n",
        "            xname=\"Epoch\"\n",
        "        )\n",
        "    })\n",
        "\n"
      ],
      "metadata": {
        "id": "ufuor-QMjG7O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "net = NeuralNet(\n",
        "    input_dim=784,\n",
        "    hidden_layers=[128, 64],\n",
        "    output_dim=10,\n",
        "    optimizer='adam',\n",
        "    lr=0.001\n",
        ")\n",
        "(X_train, Y_train), (X_test, Y_test) = get_fa_mnist(flatten=True)\n",
        "net.train(\n",
        "    X_train, Y_train,\n",
        "    X_test, Y_test,\n",
        "    epochs=10,\n",
        "    batch_size=64\n",
        ")\n",
        "\n",
        "wandb.finish()\n"
      ],
      "metadata": {
        "id": "Ji5bTPNrjH1o"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}